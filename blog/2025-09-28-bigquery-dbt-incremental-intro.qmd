---
title: "BigQuery で dbt incremental モデルのパフォーマンス改善をしてみた"
author: uma-chan
date: 2025-09-28 21:38:33 +0900
date-modified: last-modified
image: "/assets/common/icon_hhkb3_large.jpg"
description: |
  BigQuery 上で dbt incremental モデルの実行時間を40分短縮した手順を共有する
categories:
  - "tech"
  - "tech-data"
  - "tech-dbt"
---

## 1. はじめに

ログ同士を突き合わせる dbt モデルの実行時間が1時間を超えていたため、BigQuery の特性を活かしつつ実行時間を40分短縮した流れを紹介します。対象はイミューターなログを扱うモデルで、過去データの更新を考慮しない前提を置ける構造でした。

## 2. 当初の課題

- incremental_strategy が merge で、ターゲットテーブル全体を走査していた
- unique_key が15列と多く、結合条件が複雑であった
- 参照元テーブルが期間条件なしで FULL OUTER JOIN され、中間結果が肥大化していた

## 3. 実施した改善策

### 3.1. incremental_strategy の切り替え

- merge から insert_overwrite へ変更し、対象パーティションのみを差し替えた

### 3.2. パーティションプルーニングの徹底

- vars で渡した日付範囲を JOIN 前のフィルタで適用し、読み取り対象を日次で限定した
- パーティション指定を日単位で明示化し、BigQuery のスキャン範囲を事前に制限した

### 3.3. 重複除去ロジックの簡素化

- 15列のunique_key指定を ROW_NUMBER() による決定的な重複除去に変更した
- 複雑なMERGE処理からシンプルな重複除去ロジックへ簡素化した

## 4. 効果と検証

- 実行時間は1時間超えから40分短縮できた
- 同期間の surrogate key 単位で件数と代表値を比較し整合性を確認できた
- 修正前後で更新内容に差異がないことを確認できた (ログデータの特性上そうなる)

## 5. おわりに

BigQuery の挙動と dbt の incremental モデルへの習熟度がまだまだなので引き続き学びを深めます。
