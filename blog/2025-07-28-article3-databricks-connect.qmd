---
title: "Databricks Connect å®Ÿè·µç·¨ â€• ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰ Databricks ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆã‚’åˆ©ç”¨"
author: uma-chan
categories:
  - "blog"
  - "tech-data"
date: 2025-07-28 01:54:03 +0900
date-modified: last-modified
description: |
  Databricks Connect ã§ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒã‹ã‚‰ Databricks ã® Spark ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’åˆ¶å¾¡ã—ã¾ã™
image: "/assets/common/icon_hhkb3_large.jpg"
---

æœ¬è¨˜äº‹ã¯3éƒ¨æ§‹æˆã®3æœ¬ç›®ã§ã™ã€‚

1. [Claude Code å¯¾å¿œ Dev Container ç’°å¢ƒæ§‹ç¯‰ç·¨ - VS Code ã§ã‚‚ãã‚Œä»¥å¤–ã§ã‚‚](2025-07-28-article1-devcontainer.qmd)
1. [Python é–‹ç™ºç’°å¢ƒæœ€é©åŒ–ç·¨ - uv + pre-commit + GitHub Actions](2025-07-28-article2-python-env.qmd)
1. [Databricks Connect å®Ÿè·µç·¨ - ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰ Databricks ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆã‚’åˆ©ç”¨](2025-07-28-article3-databricks-connect.qmd)

## 1. ã¯ã˜ã‚ã«

å‰å›ã¾ã§ã®ç’°å¢ƒã« Databricks Connect ã‚’è¿½åŠ ã—ã¦ã€ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒã‹ã‚‰ Databricks ã® Spark ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

Databricks ä¸Šã§ã‚‚ãƒ­ãƒ¼ã‚«ãƒ«ã§ã‚‚åŒã˜ã‚³ãƒ¼ãƒ‰ãŒå‹•ä½œã™ã‚‹æ±ç”¨çš„ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚‚ç´¹ä»‹ã—ã¾ã™ã€‚

## 2. å¯¾è±¡èª­è€…

- Databricks ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰åˆ©ç”¨ã—ãŸã„æ–¹
- Databricks ä¸Šã§ã®é–‹ç™ºã« Claude Code ã‚’åˆ©ç”¨ã—ãŸã„æ–¹
- Databricks ä»¥å¤–ã® DWH ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ã£ã¦ã„ã‚‹ãŒå‚è€ƒã«ã—ãŸã„æ–¹

## 3. Databricks Connect ã¨ã¯

Databricks Connect ã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒã‹ã‚‰ Databricks ã‚¯ãƒ©ã‚¹ã‚¿ã«æ¥ç¶šã§ãã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚

ã“ã‚Œã«ã‚ˆã‚Šä»¥ä¸‹ã®ãƒ¡ãƒªãƒƒãƒˆãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚

- VS Code ã‚„ Claude Code ãªã©ã®ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒãŒåˆ©ç”¨ã§ãã‚‹
- ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®é–‹ç™ºã¨ Databricks ä¸Šã§ã®å®Ÿè¡Œã§åŒã˜ã‚³ãƒ¼ãƒ‰ãŒä½¿ãˆã‚‹

## 4. ç’°å¢ƒè¨­å®š

### 4.1. pyproject.toml ã¸ã®è¿½åŠ 

å‰å›ã® pyproject.toml ã«ä»¥ä¸‹ã®ä¾å­˜é–¢ä¿‚ã‚’è¿½åŠ ã—ã¾ã™ã€‚

```{.toml filename="pyproject.toml"}
[project]
dependencies = [
    "databricks-connect~=16.4.0",
    "ipykernel",
    "jupyterlab",
    "matplotlib",
    "numpy",
    "pandas",
    "python-dotenv",
    "requests",
    "seaborn",
]

[tool.flake8]
extend-ignore = [
    "E203",  # Whitespace before ':'
    "E701",  # Multiple statements on one line (colon)
    "F821"   # undefined name (Databricks-specific module)
]
```

### 4.2. Dev Containerè¨­å®šã®æ›´æ–°

devcontainer.json ã«ä»¥ä¸‹ã®è¨­å®šã‚’è¿½åŠ ã—ã¾ã™ã€‚

```{.json filename=".devcontainer/devcontainer.json"}
{
    "runArgs": [
        "--cap-add=NET_ADMIN",
        "--cap-add=NET_RAW",
        "--network=host"
    ],
    "mounts": [
        "source=${localEnv:HOME}/.databrickscfg,target=/home/node/.databrickscfg,type=bind,consistency=cached",
    ]
}
```

### 4.3. ç’°å¢ƒå¤‰æ•°è¨­å®š

`.env.example` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚

```{.sh filename=".env.example"}
# Databricksè¨­å®š

# .databrickscfgã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ¨å¥¨ï¼‰
DATABRICKS_CONFIG_PROFILE=DEFAULT

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½¿ç”¨ã®å ´åˆï¼ˆã©ã¡ã‚‰ã‹ä¸€æ–¹ã‚’è¨­å®šï¼‰
# DATABRICKS_CLUSTER_ID=

# Serverless Computeä½¿ç”¨ã®å ´åˆ
DATABRICKS_SERVERLESS_COMPUTE_ID=auto

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯ç„¡åŠ¹åŒ–
DATABRICKS_CONNECT_DISABLE_VERSION_CHECK=true
```

## 5. Spark ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

Dev Container (ãƒ­ãƒ¼ã‚«ãƒ«) ã¨ Databricks ã®ä¸¡æ–¹ã§åŒã˜ã‚ˆã†ã« Spark ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç”¨æ„ã—ã¾ã—ãŸã€‚

### 5.1. ä½¿ç”¨æ–¹æ³•

```{.python}
from databricks_spark import create_spark_session

# æ–°ã—ã„Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
spark = create_spark_session()
df = spark.sql("SHOW CATALOGS")
df.show()
```

### 5.2. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå®Ÿè£…

::: {.callout-note appearance="simple" collapse="true"}

#### 5.2.1. `databricks_spark.py`

```{.python filename="databricks_spark.py"}
"""
Databricks Spark ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
Databricksã¨ãƒ­ãƒ¼ã‚«ãƒ«ä¸¡å¯¾å¿œã®1ãƒ•ã‚¡ã‚¤ãƒ«å®Œçµå‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

ä½¿ç”¨æ–¹æ³•:
    from databricks_spark import create_spark_session

    # æ–°ã—ã„Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
    spark = create_spark_session()
    df = spark.sql("SHOW CATALOGS")
    df.show()
"""

import logging
import os
import sys

# ãƒ­ã‚¬ãƒ¼è¨­å®š
_logger = logging.getLogger(__name__)
if not _logger.handlers:
    _logger.setLevel(logging.INFO)
    formatter = logging.Formatter("%(asctime)s %(name)s [%(levelname)s] %(message)s")
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(formatter)
    _logger.addHandler(handler)


def is_databricks_environment() -> bool:
    """Databricksç’°å¢ƒã§å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹ã‹ã‚’åˆ¤å®š"""
    return os.environ.get("DATABRICKS_RUNTIME_VERSION") is not None


def get_environment_type() -> str:
    """å®Ÿè¡Œç’°å¢ƒã®ã‚¿ã‚¤ãƒ—ã‚’å–å¾—"""
    if is_databricks_environment():
        return "databricks"
    else:
        return "local"


def create_databricks_native_session():
    """Databricksä¸Šã§ã®ãƒã‚¤ãƒ†ã‚£ãƒ–Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
    try:
        from pyspark.sql import SparkSession

        spark = SparkSession.getActiveSession()
        if spark is None:
            spark = SparkSession.builder.appName("DatabricksNotebook").getOrCreate()

        _logger.info("âœ… Databricks native Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³å–å¾—å®Œäº†")
        _logger.info(f"ğŸ“Š Spark version: {spark.version}")
        return spark

    except ImportError:
        raise ImportError("Databricksç’°å¢ƒã§PySparkãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")


def create_local_connect_session():
    """ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®DatabricksConnect Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
    try:
        from databricks.connect import DatabricksSession

        # ç’°å¢ƒå¤‰æ•°ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
        os.environ.setdefault("DATABRICKS_CONFIG_PROFILE", "DEFAULT")
        os.environ.setdefault("DATABRICKS_SERVERLESS_COMPUTE_ID", "auto")
        os.environ.setdefault("DATABRICKS_CONNECT_DISABLE_VERSION_CHECK", "true")

        # .envèª­ã¿è¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        try:
            from dotenv import find_dotenv, load_dotenv

            env_file = find_dotenv()
            if env_file:
                load_dotenv(env_file)
                _logger.info(f"ğŸ“ .envèª­ã¿è¾¼ã¿: {env_file}")
        except ImportError:
            _logger.info("ğŸ“ dotenvãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãªã— - ç’°å¢ƒå¤‰æ•°ã®ã¿ä½¿ç”¨")

        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®šãƒã‚§ãƒƒã‚¯
        profile_name = os.environ.get("DATABRICKS_CONFIG_PROFILE")

        if profile_name:
            _logger.info(f"ğŸ”§ .databrickscfgãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨: {profile_name}")

            if os.environ.get("DATABRICKS_CLUSTER_ID"):
                cluster_id = os.environ.get("DATABRICKS_CLUSTER_ID")
                spark = (
                    DatabricksSession.builder.profile(profile_name)
                    .clusterId(cluster_id)
                    .getOrCreate()
                )
                _logger.info(f"ğŸ†” ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½¿ç”¨: {cluster_id}")
            else:
                spark = (
                    DatabricksSession.builder.profile(profile_name)
                    .serverless(True)
                    .getOrCreate()
                )
                _logger.info("ğŸš€ Serverless Computeä½¿ç”¨")
        else:
            _logger.info("ğŸ”§ ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ç›´æ¥æ¥ç¶š")

            required_vars = ["DATABRICKS_HOST", "DATABRICKS_TOKEN"]
            missing_vars = [var for var in required_vars if not os.environ.get(var)]

            if missing_vars:
                raise ValueError(f"ç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®š: {missing_vars}")

            if os.environ.get("DATABRICKS_CLUSTER_ID"):
                cluster_id = os.environ.get("DATABRICKS_CLUSTER_ID")
                spark = DatabricksSession.builder.clusterId(cluster_id).getOrCreate()
                _logger.info(f"ğŸ†” ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½¿ç”¨: {cluster_id}")
            else:
                spark = DatabricksSession.builder.serverless(True).getOrCreate()
                _logger.info("ğŸš€ Serverless Computeä½¿ç”¨")

        # DataFrameè¡¨ç¤ºæœ€é©åŒ–ï¼ˆServerlessç’°å¢ƒã§ã¯ä¸€éƒ¨è¨­å®šãŒåˆ¶é™ã•ã‚Œã‚‹ï¼‰
        try:
            spark.conf.set("spark.sql.repl.eagerEval.enabled", True)
        except Exception:
            # Serverless Computeã§ã¯è¨­å®šã§ããªã„å ´åˆãŒã‚ã‚‹ã®ã§ã‚¹ã‚­ãƒƒãƒ—
            pass

        _logger.info("âœ… Databricks Connect Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå®Œäº†")
        _logger.info(f"ğŸ“Š Spark version: {spark.version}")
        _logger.info(f"ğŸŒ æ¥ç¶šå…ˆ: {spark.client.host}")
        return spark

    except ImportError as e:
        raise ImportError(f"databricks-connectãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")


def create_spark_session():
    """å®Ÿè¡Œç’°å¢ƒã«å¿œã˜ã¦é©åˆ‡ãªSparkã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
    env_type = get_environment_type()
    _logger.info(f"ğŸ” å®Ÿè¡Œç’°å¢ƒæ¤œå‡º: {env_type}")

    try:
        if env_type == "databricks":
            return create_databricks_native_session()
        else:  # localç’°å¢ƒï¼ˆVS Codeã€Cursorã€Dev Container CLIç­‰ï¼‰
            return create_local_connect_session()

    except Exception as e:
        error_msg = str(e)
        _logger.error(f"âŒ Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå¤±æ•—: {error_msg}")

        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸æ•´åˆã®æ¤œå‡ºã¨å¯¾å¿œææ¡ˆ
        if "Unsupported combination" in error_msg and "Databricks Runtime" in error_msg:
            _logger.error("ğŸ”§ ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸æ•´åˆãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
            _logger.error("ğŸ’¡ ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®å¯¾å¿œã‚’è¡Œã£ã¦ãã ã•ã„:")
            _logger.error(
                "   1. DatabricksConnectã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã«åˆã‚ã›ã¦ãƒ€ã‚¦ãƒ³ã‚°ãƒ¬ãƒ¼ãƒ‰:"
            )
            _logger.error(
                "      uv add 'databricks-connect~=[ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ãƒãƒ¼ã‚¸ãƒ§ãƒ³]'"
            )
            _logger.error("   2. Databricksã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰")

        _logger.error(f"ğŸ’¡ ç’°å¢ƒã‚¿ã‚¤ãƒ—: {env_type}")
        raise


# æ˜ç¤ºçš„ã«create_spark_session()ã‚’å‘¼ã³å‡ºã—ã¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¦ãã ã•ã„
# ä¾‹: spark = create_spark_session()
```

:::

## 6. Databrickså´ã®è¨­å®š

### 6.1. æ¥ç¶šå…ˆã‚¯ãƒ©ã‚¹ã‚¿è¨­å®š

Spark config ã§ä»¥ä¸‹ã‚’è¨­å®šã—ã¾ã™ã€‚

```text
spark.databricks.service.server.enabled true
```

### 6.2. Databricks æ¥ç¶šè¨­å®š

Databricks ã¸æ¥ç¶šã™ã‚‹å ´åˆã¯ `~/.databrickscfg` ã«ä»¥ä¸‹ã®å†…å®¹ã‚’è¨˜è¿°ã—ã¾ã™ã€‚

```ini
[DEFAULT]
host = https://your-databricks-workspace.cloud.databricks.com
token = your-access-token
```

- `host`: Databricks ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã® URL
- `token`: Databricks ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³

## 7. åˆ©ç”¨æ‰‹é †

### 7.1. `.env` ä½œæˆ

1. `.env` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™

    ```sh
    cp .env.example .env
    ```

1. å¿…è¦ã§ã‚ã‚Œã° `.env` ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„

### 7.2. é–‹ç™ºãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

1. Dev Container ã‚’èµ·å‹•
1. Python ã‚«ãƒ¼ãƒãƒ«ã‚’é¸æŠ
1. Spark ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¦é–‹ç™ºé–‹å§‹

```python
from databricks_spark import create_spark_session

# Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
spark = create_spark_session()

# ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
df = spark.sql("SELECT * FROM your_table LIMIT 10")
df.show()

# ãƒ‡ãƒ¼ã‚¿åˆ†æ
df.groupBy("category").count().show()
```

## 8. ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã® Python ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•

Databricks ã§ã®å®Ÿè¡Œæ™‚ã®ã¿ Python ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ HACK ãªæ–¹æ³•ã§ã™ã€‚

uv ã‚’ä½¿ã£ã¦ã„ãªã„å ´åˆã¯ `%pip install <package>` ã§å¤§ä¸ˆå¤«ã§ã™ã€‚

```{.py}
import os
if os.environ.get("DATABRICKS_RUNTIME_VERSION"):
    %pip install uv
    %pip install -r <(uv pip compile pyproject.toml --color never)
```

## 9. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### 9.1. ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸æ•´åˆã‚¨ãƒ©ãƒ¼

Databricks Connect ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ã‚¯ãƒ©ã‚¹ã‚¿ã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒåˆã‚ãªã„å ´åˆãŒã‚ã‚Šã¾ã™

```sh
# ã‚¯ãƒ©ã‚¹ã‚¿ã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã«åˆã‚ã›ã¦ãƒ€ã‚¦ãƒ³ã‚°ãƒ¬ãƒ¼ãƒ‰
uv add 'databricks-connect~=14.3.0'  # ã“ã®å ´åˆã¯ DBR 14.3 ã«å¯¾å¿œ
```

### 9.2. æ¥ç¶šã‚¨ãƒ©ãƒ¼

è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã›ãšãƒã‚¦ãƒ³ãƒˆã•ã‚Œã¦ã„ãªã„å ´åˆãŒè€ƒãˆã‚‰ã‚Œã‚‹ã®ã§ã€devcontainer.json ã® `mounts` è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

## 10. ãŠã‚ã‚Šã«

ã“ã‚Œã§ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒã‹ã‚‰ Databricks ã® Spark ã‚¯ãƒ©ã‚¹ã‚¿ã‚’åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

Claude Code ã«ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å®Ÿè¡Œã¨ãƒ‡ãƒãƒƒã‚°ã‚’ä»»ã›ã‚‹ã“ã¨ã§ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã‚„æ©Ÿæ¢°å­¦ç¿’ã®ä½œæ¥­åŠ¹ç‡ãŒçˆ†ä¸ŠãŒã‚Šã§ã™ã­ï¼

Claude Code ã«ã“ã®æ‰‹ã®ä½œæ¥­ã‚’ä»»ã›ã‚‹ã¨ãã¯æ™‚é–“ãŒã‹ã‹ã‚‹ã®ã§ä¸¦è¡Œã—ã¦ä»–ã®ä½œæ¥­ã«å–ã‚Šçµ„ã‚€ã®ãŒã‚ªã‚¹ã‚¹ãƒ¡ã§ã™ã€‚

## 11. (ãŠã¾ã‘) Claude Code ã«èª­ã¾ã›ã‚‹ã¨ä¾¿åˆ©ãªãƒ«ãƒ¼ãƒ«

```{.md filename="CONTRIBUTING.md"}
# CONTRIBUTING

## é‡è¦ãªãƒ«ãƒ¼ãƒ«

### pre-commit è¨­å®šã«ã¤ã„ã¦

- **NEVER**: pre-commit ã‚’ç„¡åŠ¹åŒ–ã—ã¦ã¯ãªã‚‰ãªã„
- **NEVER**: `pre-commit skip` ã‚„ `git commit --no-verify` ã‚’ä½¿ç”¨ã—ã¦ã¯ãªã‚‰ãªã„
- **IMPORTANT**: pre-commit ã®ãƒã‚§ãƒƒã‚¯ã«å¤±æ•—ã—ãŸå ´åˆã¯ã€å¿…ãšã‚³ãƒ¼ãƒ‰ã‚’ä¿®æ­£ã—ã¦ã‹ã‚‰ã‚³ãƒŸãƒƒãƒˆã™ã‚‹

## Jupyter Notebook å®Ÿè¡Œæ–¹æ³•

### ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å®Ÿè¡Œæ–¹æ³•

Notebookå…¨ä½“ã‚’å®Ÿè¡Œã™ã‚‹æŒ‡ç¤ºã‚’å—ã‘ãŸéš›ã¯ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹

`uv run jupyter nbconvert --to notebook --execute <notebook_path> --inplace --ExecutePreprocessor.timeout=300`

#### ä½¿ç”¨ä¾‹

`uv run jupyter nbconvert --to notebook --execute /workspace/notebooks/databricks-connect-sample.ipynb --inplace --ExecutePreprocessor.timeout=300`

#### ã‚ªãƒ—ã‚·ãƒ§ãƒ³èª¬æ˜

- `--to notebook`: Notebookå½¢å¼ã§å‡ºåŠ›
- `--execute`: ã‚»ãƒ«ã‚’å®Ÿéš›ã«å®Ÿè¡Œ
- `--inplace`: å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã«å®Ÿè¡Œçµæœã‚’ä¸Šæ›¸ã
- `--ExecutePreprocessor.timeout=300`: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’300ç§’ã«è¨­å®š

### å®Ÿè¡Œãƒ­ã‚°ã®ç¢ºèª

å®Ÿè¡Œæ™‚ã®ãƒ­ã‚°ã‚’ç¢ºèªã—ãŸã„å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®Ÿè¡Œã™ã‚‹

`uv run jupyter nbconvert --to notebook --execute <notebook_path> --inplace --ExecutePreprocessor.timeout=300 2>&1 | tee /tmp/notebook_execution.log`

### æ³¨æ„äº‹é …

- å®Ÿè¡Œå‰ã«å¿…è¦ãªç’°å¢ƒå¤‰æ•°ï¼ˆ`.env`ãƒ•ã‚¡ã‚¤ãƒ«ç­‰ï¼‰ãŒé©åˆ‡ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹
- é•·æ™‚é–“å®Ÿè¡Œã•ã‚Œã‚‹ã‚»ãƒ«ãŒã‚ã‚‹å ´åˆã¯`--ExecutePreprocessor.timeout`ã®å€¤ã‚’èª¿æ•´ã™ã‚‹
- VS Codeã§é–‹ã„ã¦ã„ã‚‹å ´åˆã¯å®Ÿè¡Œå¾Œã«ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ã‚’ç¢ºèªã™ã‚‹
```
