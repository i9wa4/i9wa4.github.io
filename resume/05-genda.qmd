---
title: "株式会社GENDA"
categories:
  - "正社員"
date-modified: last-modified
description: |
  データエンジニア・MLOps エンジニアとして以下に従事<br>
  - データパイプライン全体の変更対応<br>
  - データマート整備<br>
  - ダッシュボード作成・運用<br>
  - AWS から Databricks への機械学習開発基盤移行対応<br>
  - CI 導入など機械学習基盤の整備
Period: "2024/11 - 現在"
---

:会社概要・雇用形態 (2024年時点) {#tbl-company tbl-colwidths="[20,80]"}

|              |                                            |
| :---         | :---                                       |
| 事業内容     | グループ各社の事業成長の支援および経営管理 |
| 資本金       | 197億6,465万円（資本剰余金含む）           |
| 売上高       | 連結：556億円（2024年1月期）               |
| 従業員数     | 連結：12,056名（2024年8月31日時点）        |
| 上場／非上場 | 上場                                       |
| 雇用形態     | 正社員                                     |
| 勤務地       | 東京都港区 (フルリモート・月1出社)         |
| 所属         | [2024/11 - 現在] IT戦略部 データチーム     |

## 1. [データ] カラオケ事業会社のデータ利活用推進

### 1.1. 概要

- 様々な意思決定をデータに基づいて行うために必要なデータ基盤の構築運用とデータ活用方法の提案

### 1.2. 期間

- 2024/11 - 現在

### 1.3. 規模・役割

- 役割：リーダー
- 規模：4名
    - データエンジニアリング全般：2名 (自分含む)
        - 内訳
            - データパイプライン構築：メンバー1名
            - dbt によるデータマート構築：私1名
            - ダッシュボード作成：私1名
    - 事業会社側役員との折衝等担当 (BizDev)：1名
    - 事業会社側経理担当：1名

### 1.4. 担当業務

- POS データやアプリDBのデータを Snowflake に集約するデータパイプライン構築・運用
    - 補足：Snowflake に転送後 Databricks からテーブルを参照しています
- 用途に応じた dbt によるデータマート構築
- データ基盤上のデータを利用したダッシュボード作成・運用
- データ基盤 (Databricks) やデータ利活用方法の啓蒙

### 1.5. 機能開発・実装詳細

- データパイプライン構築
    - MWAA を用いて事業会社側の SFTP サーバ上の CSV ファイルをダウンロードし Snowflake のテーブルへデータを追加する
    - データパイプライン全体を Terraform で管理している
    - あるいは直接受領したファイルを Snowflake へ手動追加する
- dbt によるデータマート構築
    - Databricks で dbt Core ジョブを実行し Databricks 上にデータマートを構築する
    - intermediate 層の構築により複数の mart 層から共通ロジックを切り出し保守性を向上
    - クーポン使用履歴×会計データ結合マートの構築
    - IPコラボ分析用データマートの構築
    - 年代別×時間帯別売上分析マートの構築
- データ品質監視
    - 日次自動連携データの欠損を検知し Slack 通知する仕組みを構築
- ダッシュボード作成
    - Databricks のダッシュボード機能を利用する

### 1.6. 目的・背景

- 開発が生じる要因
    - データパイプライン構築
        - M&A によりグループインした企業の POS データを新たに取り込む
        - 目標数値など連携対象が増加した際に適宜取り込む
        - IPコラボの情報などのマスターデータを営業サイドから受領する
    - dbt によるデータマート構築
        - ダッシュボード作成に必要なデータ集計処理時間の短縮のために dbt によるバッチ処理に切り出す
    - ダッシュボード作成
        - 料金体系の変更など施策の効果を監視するために可視化したいデータの要望を受ける

### 1.7. 課題

- 人員不足による開発遅延
    - 私がデータ転送部分の開発・改修を自分の手で直接実施はせず他メンバーに任せるという業務分担状況で、なおかつグループインする企業が増え続けている都合でデータ転送部分を担うデータエンジニアの手が回りづらい状況が続いていました。
    - 経営目線で求められるダッシュボードを作成する上で必要なデータへの知見や技術をもった人間が私しかいないため私がボトルネックになっています。

### 1.8. 工夫した点

> - 私がデータ転送部分の開発・改修に深くコミットしていないという業務分担状況で、なおかつグループインする企業が増え続けている都合でデータ転送部分を担うデータエンジニアの手が回りづらい状況が続いている

後段の開発作業の工数を削減する目的で、現状の実装から最小限の変更で対応できる要件となるよう事業会社側と交渉をまとめました。
実装作業こそ自分で実施しないものの、実装内容を100%理解できているため業務の切り出しを最適な形で行うことができました。

> - 経営目線で求められるダッシュボードを作成する上で必要なデータへの知見や技術をもった人間が私しかいないため私がボトルネックになっています。

現状も続いている課題であって大きな成果はまだ出せていません。
現在事業会社側では全社的にトップダウンな意思決定を伴いつつDX推進中になるため、事業会社側のメンバーの工数の確保をトップダウンで実施してもらい、その時間を活用して私の手で人材育成を行う流れを検討しています。
生成AIを補助に利用する方法などを提示しつつできる形で関与できるメンバーを増やしていければと思います。

### 1.9. 成果

- ダッシュボード
    - 事業会社側の経理部門により Excel で毎日作成されていた売上速報をダッシュボードにより完全自動化できました。
    - 週次で実施している全国のエリア統括者と役員の参加する営業会議向けに作成していた週報の作成をほぼ自動化できました。
    - 事業会社側の担当者から追跡したいデータに関する要望が出てくるようになり、データドリブンな意思決定をできる体質に一歩近付きました。
- データ基盤
    - マーケティング施策の効果検証に利用するデータを揃えることで効果的なマーケティング施策立案に貢献しました。
- データマート
    - intermediate 層の活用により dbt モデルの保守性が向上し、チームメンバーによる改修が容易になりました。
    - IPコラボ分析用データマートの提供により、コラボ施策の効果検証が可能になりました。

### 1.10. 担当フェーズ

- ヒアリングや提案による以下の要件整理
    - データパイプライン改修
    - ダッシュボード新規作成・項目追加
- 開発指揮
    - データパイプライン改修
- 開発作業
    - dbt によるデータマート構築
    - ダッシュボード新規作成・項目追加
- 運用作業
    - データパイプライン不通時の原因調査
    - データマートやダッシュボードの障害解消

### 1.11. 開発環境

- GitHub
- AWS
    - MWAA
- Snowflake
- Databricks
- dbt Core
- Terraform

## 2. [データ] ゲームセンター事業会社のIPマスターデータ作成支援

※IP：知的財産 (Intellectual Property)。ゲームやアニメのタイトルなどを指しています。

### 2.1. 概要

- IP名、IP・キャラクターの関係性、景品・IP・キャラクターの関係性を整理したマスターデータを作成する

### 2.2. 期間

- 2025/04 - 現在

### 2.3. 規模・役割

- 役割：リーダー
- 規模：4名
    - マスターデータ設計・システム設計：私1名
    - kintone アプリ PoC 担当：1名
    - 事業会社側メンバーとの調整担当兼 kintone アプリ実装担当：1名
    - データパイプライン構築：1名

### 2.4. 担当業務

- データ入力用の kintone アプリ設計
- kintone アプリからデータ基盤への連携に必要なデータパイプライン設計
- kintone アプリの運用方法検討

### 2.5. 機能開発・実装詳細

- kintone アプリ
    - IP名、IP・キャラクターの関係性、景品・IP・キャラクターの関係性を表すマスターデータへのデータ追加・修正を行う
- データパイプライン
    - kintone アプリから Snowflake へのデータ転送を行う
    - データパイプライン全体を Terraform で管理する

### 2.6. 目的・背景

- IPやキャラクターを包括的に管理する仕組みが社内外ともに存在せず、景品選定も担当者の経験則に依存しており改善が求められる状況でした。
- 景品の売上実績やIPの流行に関する情報をデータとして蓄積していき更に景品選定に活用するサイクルを作っていきたい狙いがあります。

### 2.7. 課題

- 以下の要求があり、入力からデータ基盤への連携までどのようなシステムが最適か検討する必要がありました。
    - 景品を卸す業者に一次情報を入力してもらう kintone アプリの導入が進められている最中であった
    - 責任分掌の観点からデータの修正や管理を調達部門に自律的に実施してもらう必要がある
    - 毎日数十件のデータ入力が必要でありデータ入力担当者の負担を極力軽減させたい
- 当初利便性を考慮して Notion によるデータ入力を検討していました。Notion では誤入力が起こりやすいという欠点があるため Notion 上の仮のマスターデータと、データ基盤上に別途真のマスターデータを用意しておき、レビューを行った後に両者を同期するという運用を検討しました。
    - この構成の場合レビュー工程が都度入ることで入力担当者の負担が大きくなることが予想されました。

### 2.8. 工夫した点

- kintone アプリの仕様を調査していくうちに入力データへのバリデーションを設定でき誤入力を防げることが分かりました。これにより当初考慮していなかったマスターデータ管理用社内 kintone アプリの導入を決めました。
    - この方向転換によって課題をほぼ全て解消できました。

### 2.9. 成果

- 開発中

### 2.10. 担当フェーズ

- マスターデータの設計
- システム全体の要件定義・設計

### 2.11. 開発環境

- データ入力アプリ
    - kintone
- データ基盤
    - GitHub
    - AWS
        - MWAA
    - Snowflake
    - Terraform

## 3. [MLOps] 機械学習開発運用環境の改善

### 3.1. 概要

- 社内で運用されている機械学習モデルの開発・運用ともに継続改善を行う

### 3.2. 期間

- 2024/11
- 2025/03 - 現在

### 3.3. 規模・役割

- 役割：リーダー
- 規模：私1名

### 3.4. 担当業務

- 機械学習関連の改善策立案・設計・実装
- データチーム全体の CI/CD 改善

### 3.5. 機能開発・実装詳細

- リポジトリへの CI/CD 導入
    - Linter/Formatter を導入し Python/SQL/Terraform のコード品質を確保する
    - black, isort, flake8 から Ruff への移行によりツールを統合し CI 実行時間を短縮
    - mise による開発ツールのバージョン管理を導入し環境構築を簡素化
    - Staging/Production へのデプロイを GitHub Actions の手動実行で行えるようにする
- デプロイ手順改善
    - Production 環境へのデプロイ実行はレビューを伴う形へと改善する
    - リリースタグを用いた標準的なリリース管理を行う運用手順を整備する
- スコアリング結果のプロダクトへの連携システム構成の変更
    - スコアリングをプロダクトアプリと同じ AWS アカウント内の ECS で実施していた構成から、より開発をしやすくスケーラビリティの確保が容易な Databricks でのスコアリング実施に変更する
- [検討中] 機械学習に利用するデータのデータマート化
- [検討中] プロダクトへのデータ反映確認の自動化
- [検討中] 学習データの増加に対応できるような処理改善
- [検討中] 機械学習モデルの精度を継続監視する仕組みの導入
- [検討中] テストの導入
- [検討中] モノレポ化

### 3.6. 目的・背景

- 取り扱う事業の増加・学習データの増加に対応していくために開発・運用を改善が求められている

### 3.7. 課題

- 数名のインターン生にロジック部分の開発に集中してもらうことで成果を出そうとしている中で、開発・運用に手作業が多く含まれることからコントリビューションの難易度が高くなってしまっている
- 対象事業のアクティブユーザー数が増加することで学習データが増加しシステムのスケールが必要となる
- 対象事業が増加する際の工数増加を懸念して新たな機械学習活用を提案しづらい状況にある

### 3.8. 工夫した点

- まずできる所から改善を進めていき1年かけてゴールを目指すというマイルストーンを立て、恩恵を実感してもらいつつ協力を得ながら進める方針を取った点

### 3.9. 成果

- データサイエンスチームで最も注力しているオンラインクレーン事業の景品レコメンドシステムに関して CI/CD 導入・デプロイ手順改善を行いインターン生のコントリビューション難易度を下げることに成功しています。
- Ruff への移行により複数の Linter/Formatter を統合し、CI パイプラインの実行時間を短縮しました。
- Databricks 上での機械学習環境構築の知見を社外登壇で共有しました。
    - [試されDATA SAPPORO #1](https://tamesaredatahokkaido.connpass.com/event/369741/): GENDA の機械学習環境を AWS から Databricks に移行してみた
    - [Databricks Data + AI World Tour Tokyo After Party](https://events.databricks.com/daiwt-tokyo-after-party/registration-closed): Databricks 向け Jupyter Kernel でデータサイエンティストの開発環境を AI-Ready にする
- Databricks 向け Jupyter Kernel ([jupyter-databricks-kernel](https://github.com/nicholashm/jupyter-databricks-kernel)) の OSS 開発に貢献しました。

### 3.10. 担当フェーズ

- 機械学習関連の改善策立案・設計・実装

### 3.11. 開発環境

- GitHub
- GitHub Actions
- AWS
    - ECS
    - RDS
    - S3
- Databricks
- dbt Core
- Ruff
- mise

## 4. [その他] 開発環境の AI-Ready 化推進

### 4.1. 概要

- AI コーディングツールを活用した開発生産性向上の推進
- データサイエンティスト向け開発環境の AI-Ready 化

### 4.2. 期間

- 2024/11 - 現在

### 4.3. 規模・役割

- 役割：推進者
- 規模：私1名 (チーム全体への展開)

### 4.4. 担当業務

- AI コーディングツールの調査・導入・活用推進
- データサイエンティスト開発環境の AI-Ready 化設計・実装
- チームメンバーへの知見共有・啓蒙

### 4.5. 取り組み内容

- AI コーディングツールの導入
    - Claude Code / Codex CLI をメインツールとして採用
    - Dev Container 環境を Claude Code 対応に整備
    - Skills / Hooks / Rules を活用したルール最適化
- データサイエンティスト開発環境の AI-Ready 化
    - Databricks Notebook 開発環境を AI エージェントが扱いやすい構成に刷新
    - jupyter-databricks-kernel の導入によりローカル Jupyter から Databricks クラスタに接続可能に
    - mise + pre-commit + uv による開発ツールチェーンの標準化
    - Ruff による Notebook のコード品質担保
- MCP (Model Context Protocol) の活用
    - Databricks MCP Server の改良 (Service Principal 認証対応、機能追加)
    - AI エージェントが Databricks 上でデータ分析を実行できる環境を整備
- AI レビューシステムの構築
    - Claude Code のサブエージェントを活用することで手元でコードレビューを実施する仕組みを共有

### 4.6. 目的・背景

- データサイエンティストの開発環境は Databricks Notebook が中心だが、AI コーディングツールとの相性が悪く生産性向上の恩恵を受けにくい状況があった
- AI エージェントが効果的に動作するには、コードの構造化・Linter/Formatter の整備・ローカル実行環境の確保が必要

### 4.7. 課題

- Databricks Notebook は Web UI 中心で AI コーディングツールが直接操作しづらい
- Notebook 形式 (.ipynb) は AI エージェントにとって扱いづらいフォーマット
- チームメンバーの開発環境がバラバラで標準化されていなかった

### 4.8. 工夫した点

- AI エージェントが扱いやすい開発環境とは何かを言語化し、必要な要素を整理した上で段階的に導入した
- 自分自身が率先して AI ツールを活用し、その知見をテックブログや社内共有で発信することで導入のハードルを下げた

### 4.9. 成果

- データサイエンティストが AI コーディングツールを活用できる開発環境を整備
- Databricks MCP Server や jupyter-databricks-kernel の開発により OSS コミュニティに貢献
- 社外登壇・テックブログ執筆を通じて知見を適宜発信
    - [Cursorのおすすめ設定 & Cursorにデータ分析を任せる方法](https://i9wa4.github.io/slides/2025-05-22-midas-cursor-tips.html) (グループ会社向け勉強会)
    - [Databricks Notebook 開発環境を AI-Ready にする - Zenn](https://zenn.dev/genda_jp/articles/2025-12-19-databricks-notebook-ai-ready)

### 4.10. 利用ツール

- AI コーディングツール
    - Claude Code
    - Codex CLI
    - GitHub Copilot (コード補完, レビュー)
- 開発環境
    - tmux + Vim/Neovim
    - mise + pre-commit + uv
- Databricks 連携
    - jupyter-databricks-kernel
    - Databricks MCP Server

## 5. [その他] 開発業務以外の活動

### 5.1. 記事執筆・登壇等

下記ページをご確認ください。

[通常業務以外の活動 | 職務経歴書](index.qmd#通常業務以外の活動)

### 5.2. チーム育成

- メンバーへの技術指導
    - Issue を作成し AWS / Terraform の技術指導を実施
    - デイリー MTG で進捗状況を把握しながら適宜技術的なアドバイスを提供
    - コードレビューを通じた品質向上と技術力向上の支援
- 1on1 の実施
    - 定期的な 1on1 を通じてメンバーの課題や成長目標を把握し支援

### 5.3. 採用活動

- カジュアル面談実施
    - SNS にてカジュアル面談を実施している旨宣伝を行い面談に繋げました。
- 転職媒体での候補者選定
    - 自主的に採用業務への関与を申し出て以降、現在まで継続的に候補者選定業務を担当しております。
